{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolLM2 with SFTTrainer</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the `HuggingFaceTB/smoltalk` dataset</p>\n",
    "    <p>üêï Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`.</p>\n",
    "    <p>ü¶Å Select a dataset that relates to a real world use case your interested in</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-smoltalk\"\n",
    "finetune_tags = [\"smol-course\", \"SFT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with the base model\n",
    "\n",
    "Here we will try out the base model which does not have a chat template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a haiku about programming.\n",
      "\n",
      "A haiku is a short poem that consists of just three lines. The first line is called the ‚Äúzenzen‚Äù and is usually about nature. The second line is called the ‚Äúten-zenzen‚Äù and is about the beauty of nature. The third line is called the ‚Äú5-zenzen‚Äù and is about the beauty of nature.\n",
      "\n",
      "A haiku is a very simple poem. It is a way of expressing a feeling or a thought. It is a way\n"
     ]
    }
   ],
   "source": [
    "# # pip install transformers\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "# device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "# inputs = tokenizer.encode(\"Write a haiku about programming\", return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs, max_new_tokens=100)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_topic': 'Travel/Vacation destinations/Beach resorts',\n",
       " 'messages': [{'content': 'Hi there', 'role': 'user'},\n",
       "  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       "  {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Okay, I'll look into those. Thanks for the recommendations!\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds[\"train\"]\n",
    "ds_test = ds[\"test\"]\n",
    "\n",
    "def process_dataset(sample):\n",
    "    sample[\"tokenized_input\"] = tokenizer.apply_chat_template(sample[\"messages\"], tokenize=True, add_generation_prompt=True)\n",
    "    return sample\n",
    "\n",
    "ds_train_processed = ds_train.map(process_dataset)\n",
    "ds_test_processed = ds_test.map(process_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_topic': 'Travel/Vacation destinations/Beach resorts',\n",
       " 'messages': [{'content': 'Hi there', 'role': 'user'},\n",
       "  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       "  {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Okay, I'll look into those. Thanks for the recommendations!\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\",\n",
       "   'role': 'assistant'}],\n",
       " 'tokenized_input': [1,\n",
       "  4093,\n",
       "  198,\n",
       "  26843,\n",
       "  665,\n",
       "  2,\n",
       "  198,\n",
       "  1,\n",
       "  520,\n",
       "  9531,\n",
       "  198,\n",
       "  19556,\n",
       "  17,\n",
       "  1073,\n",
       "  416,\n",
       "  339,\n",
       "  724,\n",
       "  346,\n",
       "  1834,\n",
       "  47,\n",
       "  2,\n",
       "  198,\n",
       "  1,\n",
       "  4093,\n",
       "  198,\n",
       "  57,\n",
       "  5248,\n",
       "  3012,\n",
       "  327,\n",
       "  253,\n",
       "  10724,\n",
       "  14500,\n",
       "  327,\n",
       "  957,\n",
       "  1867,\n",
       "  17584,\n",
       "  30,\n",
       "  1978,\n",
       "  346,\n",
       "  3730,\n",
       "  634,\n",
       "  2378,\n",
       "  2911,\n",
       "  47,\n",
       "  2,\n",
       "  198,\n",
       "  1,\n",
       "  520,\n",
       "  9531,\n",
       "  198,\n",
       "  4449,\n",
       "  2378,\n",
       "  10724,\n",
       "  36088,\n",
       "  1453,\n",
       "  48326,\n",
       "  281,\n",
       "  14126,\n",
       "  28,\n",
       "  260,\n",
       "  48148,\n",
       "  898,\n",
       "  28,\n",
       "  284,\n",
       "  260,\n",
       "  44057,\n",
       "  30,\n",
       "  1069,\n",
       "  2316,\n",
       "  1343,\n",
       "  327,\n",
       "  480,\n",
       "  3953,\n",
       "  16351,\n",
       "  284,\n",
       "  13253,\n",
       "  29,\n",
       "  10086,\n",
       "  5656,\n",
       "  30,\n",
       "  2,\n",
       "  198,\n",
       "  1,\n",
       "  4093,\n",
       "  198,\n",
       "  5195,\n",
       "  4598,\n",
       "  1109,\n",
       "  30,\n",
       "  4184,\n",
       "  665,\n",
       "  750,\n",
       "  36088,\n",
       "  281,\n",
       "  260,\n",
       "  11981,\n",
       "  338,\n",
       "  359,\n",
       "  1123,\n",
       "  327,\n",
       "  3168,\n",
       "  47,\n",
       "  2,\n",
       "  198,\n",
       "  1,\n",
       "  520,\n",
       "  9531,\n",
       "  198,\n",
       "  10539,\n",
       "  28,\n",
       "  260,\n",
       "  25518,\n",
       "  284,\n",
       "  7784,\n",
       "  48096,\n",
       "  10015,\n",
       "  284,\n",
       "  47557,\n",
       "  395,\n",
       "  359,\n",
       "  5412,\n",
       "  4975,\n",
       "  327,\n",
       "  1564,\n",
       "  29,\n",
       "  9263,\n",
       "  36088,\n",
       "  281,\n",
       "  260,\n",
       "  11981,\n",
       "  30,\n",
       "  1069,\n",
       "  2626,\n",
       "  253,\n",
       "  1845,\n",
       "  282,\n",
       "  2123,\n",
       "  284,\n",
       "  32255,\n",
       "  5712,\n",
       "  327,\n",
       "  511,\n",
       "  6399,\n",
       "  30,\n",
       "  2,\n",
       "  198,\n",
       "  1,\n",
       "  4093,\n",
       "  198,\n",
       "  39122,\n",
       "  28,\n",
       "  339,\n",
       "  3060,\n",
       "  1492,\n",
       "  618,\n",
       "  967,\n",
       "  30,\n",
       "  10090,\n",
       "  327,\n",
       "  260,\n",
       "  7400,\n",
       "  17,\n",
       "  2,\n",
       "  198,\n",
       "  1,\n",
       "  520,\n",
       "  9531,\n",
       "  198,\n",
       "  2683,\n",
       "  2316,\n",
       "  10668,\n",
       "  30,\n",
       "  339,\n",
       "  3826,\n",
       "  346,\n",
       "  1042,\n",
       "  260,\n",
       "  3468,\n",
       "  14500,\n",
       "  327,\n",
       "  469,\n",
       "  17584,\n",
       "  30,\n",
       "  2,\n",
       "  198,\n",
       "  1,\n",
       "  520,\n",
       "  9531,\n",
       "  198]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the SFTTrainer\n",
    "\n",
    "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e55dfdeaaa4860a6b14718c94db5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d8b0d0ac744bc8a4accad79fcd9e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=200,  # Frequency of saving model checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=100,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False # Use MPS for mixed precision training\n",
    "    ),\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    "    max_seq_length=1024,  # Maximum input length for the model\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds_train_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds_test_processed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb1230f455b4ccdb244173fab096eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6774, 'grad_norm': 2.720546007156372, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2563, 'grad_norm': 2.6445446014404297, 'learning_rate': 4.9e-05, 'epoch': 0.04}\n",
      "{'loss': 1.234, 'grad_norm': 2.3947601318359375, 'learning_rate': 4.85e-05, 'epoch': 0.05}\n",
      "{'loss': 1.161, 'grad_norm': 2.502878427505493, 'learning_rate': 4.8e-05, 'epoch': 0.07}\n",
      "{'loss': 1.0657, 'grad_norm': 2.2998220920562744, 'learning_rate': 4.75e-05, 'epoch': 0.09}\n",
      "{'loss': 1.1566, 'grad_norm': 2.2886576652526855, 'learning_rate': 4.7e-05, 'epoch': 0.11}\n",
      "{'loss': 1.1648, 'grad_norm': 2.012474298477173, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.12}\n",
      "{'loss': 1.1244, 'grad_norm': 2.128523111343384, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 1.086, 'grad_norm': 2.224428415298462, 'learning_rate': 4.55e-05, 'epoch': 0.16}\n",
      "{'loss': 1.1116, 'grad_norm': 2.0808632373809814, 'learning_rate': 4.5e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed34549df6c648e4bab900655e6f883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1240651607513428, 'eval_runtime': 4.1417, 'eval_samples_per_second': 28.732, 'eval_steps_per_second': 3.622, 'epoch': 0.18}\n",
      "{'loss': 1.0621, 'grad_norm': 2.050333261489868, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.19}\n",
      "{'loss': 1.1234, 'grad_norm': 2.0779809951782227, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.21}\n",
      "{'loss': 1.0921, 'grad_norm': 1.9968067407608032, 'learning_rate': 4.35e-05, 'epoch': 0.23}\n",
      "{'loss': 1.0627, 'grad_norm': 2.1300883293151855, 'learning_rate': 4.3e-05, 'epoch': 0.25}\n",
      "{'loss': 1.0624, 'grad_norm': 2.0639355182647705, 'learning_rate': 4.25e-05, 'epoch': 0.27}\n",
      "{'loss': 1.0679, 'grad_norm': 2.0235002040863037, 'learning_rate': 4.2e-05, 'epoch': 0.28}\n",
      "{'loss': 1.0966, 'grad_norm': 2.064934253692627, 'learning_rate': 4.15e-05, 'epoch': 0.3}\n",
      "{'loss': 1.0368, 'grad_norm': 1.7581497430801392, 'learning_rate': 4.1e-05, 'epoch': 0.32}\n",
      "{'loss': 1.0548, 'grad_norm': 1.9994392395019531, 'learning_rate': 4.05e-05, 'epoch': 0.34}\n",
      "{'loss': 1.0482, 'grad_norm': 1.8691847324371338, 'learning_rate': 4e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcea60746f94d9ea9d1709c190ccc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0796984434127808, 'eval_runtime': 4.2533, 'eval_samples_per_second': 27.978, 'eval_steps_per_second': 3.527, 'epoch': 0.35}\n",
      "{'loss': 1.0709, 'grad_norm': 1.9666708707809448, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.37}\n",
      "{'loss': 1.1066, 'grad_norm': 2.0240750312805176, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.39}\n",
      "{'loss': 1.0444, 'grad_norm': 1.9775208234786987, 'learning_rate': 3.85e-05, 'epoch': 0.41}\n",
      "{'loss': 1.0338, 'grad_norm': 1.9441879987716675, 'learning_rate': 3.8e-05, 'epoch': 0.42}\n",
      "{'loss': 1.0412, 'grad_norm': 1.9525994062423706, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.44}\n",
      "{'loss': 1.0647, 'grad_norm': 1.8651009798049927, 'learning_rate': 3.7e-05, 'epoch': 0.46}\n",
      "{'loss': 1.0536, 'grad_norm': 1.890933632850647, 'learning_rate': 3.65e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0613, 'grad_norm': 1.9998323917388916, 'learning_rate': 3.6e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0283, 'grad_norm': 1.873939871788025, 'learning_rate': 3.55e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0292, 'grad_norm': 1.8299031257629395, 'learning_rate': 3.5e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40ebb2f675e4f3ea84566cc3e176ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0614721775054932, 'eval_runtime': 3.9924, 'eval_samples_per_second': 29.806, 'eval_steps_per_second': 3.757, 'epoch': 0.53}\n",
      "{'loss': 1.0241, 'grad_norm': 2.1467652320861816, 'learning_rate': 3.45e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0228, 'grad_norm': 1.9443814754486084, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.57}\n",
      "{'loss': 1.076, 'grad_norm': 1.9639250040054321, 'learning_rate': 3.35e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0508, 'grad_norm': 1.8711204528808594, 'learning_rate': 3.3e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0034, 'grad_norm': 2.0145821571350098, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0558, 'grad_norm': 1.7782917022705078, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.64}\n",
      "{'loss': 1.073, 'grad_norm': 2.164621591567993, 'learning_rate': 3.15e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0177, 'grad_norm': 2.0233154296875, 'learning_rate': 3.1e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9862, 'grad_norm': 1.7660857439041138, 'learning_rate': 3.05e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0065, 'grad_norm': 1.9037498235702515, 'learning_rate': 3e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2c055c25e3405e98db6942fdabfa2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0507944822311401, 'eval_runtime': 4.1103, 'eval_samples_per_second': 28.952, 'eval_steps_per_second': 3.649, 'epoch': 0.71}\n",
      "{'loss': 1.0134, 'grad_norm': 1.9174423217773438, 'learning_rate': 2.95e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9992, 'grad_norm': 2.0372278690338135, 'learning_rate': 2.9e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0716, 'grad_norm': 2.0216338634490967, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0657, 'grad_norm': 1.9803192615509033, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0211, 'grad_norm': 1.8331948518753052, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0586, 'grad_norm': 2.046077251434326, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0204, 'grad_norm': 1.9826030731201172, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0521, 'grad_norm': 2.020400047302246, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0505, 'grad_norm': 1.788346290588379, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.87}\n",
      "{'loss': 1.0762, 'grad_norm': 2.0216751098632812, 'learning_rate': 2.5e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db028fefff8843b493bae6f2a5142481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.033725380897522, 'eval_runtime': 3.8202, 'eval_samples_per_second': 31.151, 'eval_steps_per_second': 3.927, 'epoch': 0.88}\n",
      "{'loss': 1.0084, 'grad_norm': 1.8724583387374878, 'learning_rate': 2.45e-05, 'epoch': 0.9}\n",
      "{'loss': 1.0324, 'grad_norm': 2.0852150917053223, 'learning_rate': 2.4e-05, 'epoch': 0.92}\n",
      "{'loss': 0.9957, 'grad_norm': 1.7872930765151978, 'learning_rate': 2.35e-05, 'epoch': 0.94}\n",
      "{'loss': 1.0472, 'grad_norm': 1.9167600870132446, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.96}\n",
      "{'loss': 0.9907, 'grad_norm': 1.8367406129837036, 'learning_rate': 2.25e-05, 'epoch': 0.97}\n",
      "{'loss': 0.9813, 'grad_norm': 1.909349799156189, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.99}\n",
      "{'loss': 0.8858, 'grad_norm': 1.7717057466506958, 'learning_rate': 2.15e-05, 'epoch': 1.01}\n",
      "{'loss': 0.8192, 'grad_norm': 1.969794511795044, 'learning_rate': 2.1e-05, 'epoch': 1.03}\n",
      "{'loss': 0.7688, 'grad_norm': 1.7706958055496216, 'learning_rate': 2.05e-05, 'epoch': 1.04}\n",
      "{'loss': 0.8018, 'grad_norm': 1.7979650497436523, 'learning_rate': 2e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc91d8779f54e6ea76738247a41b8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0330913066864014, 'eval_runtime': 3.9705, 'eval_samples_per_second': 29.971, 'eval_steps_per_second': 3.778, 'epoch': 1.06}\n",
      "{'loss': 0.8795, 'grad_norm': 1.880010724067688, 'learning_rate': 1.9500000000000003e-05, 'epoch': 1.08}\n",
      "{'loss': 0.8653, 'grad_norm': 1.7579153776168823, 'learning_rate': 1.9e-05, 'epoch': 1.1}\n",
      "{'loss': 0.8589, 'grad_norm': 1.6871017217636108, 'learning_rate': 1.85e-05, 'epoch': 1.12}\n",
      "{'loss': 0.8052, 'grad_norm': 1.5876790285110474, 'learning_rate': 1.8e-05, 'epoch': 1.13}\n",
      "{'loss': 0.812, 'grad_norm': 1.807350754737854, 'learning_rate': 1.75e-05, 'epoch': 1.15}\n",
      "{'loss': 0.8324, 'grad_norm': 1.810829997062683, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.17}\n",
      "{'loss': 0.7995, 'grad_norm': 1.8692257404327393, 'learning_rate': 1.65e-05, 'epoch': 1.19}\n",
      "{'loss': 0.804, 'grad_norm': 1.757348656654358, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.2}\n",
      "{'loss': 0.8219, 'grad_norm': 1.7896767854690552, 'learning_rate': 1.55e-05, 'epoch': 1.22}\n",
      "{'loss': 0.7538, 'grad_norm': 2.0682058334350586, 'learning_rate': 1.5e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a757b77162478e9b55cf38bc0ec8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0311486721038818, 'eval_runtime': 4.007, 'eval_samples_per_second': 29.698, 'eval_steps_per_second': 3.743, 'epoch': 1.24}\n",
      "{'loss': 0.7949, 'grad_norm': 1.847456693649292, 'learning_rate': 1.45e-05, 'epoch': 1.26}\n",
      "{'loss': 0.8761, 'grad_norm': 1.949364185333252, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.27}\n",
      "{'loss': 0.8179, 'grad_norm': 1.7645949125289917, 'learning_rate': 1.3500000000000001e-05, 'epoch': 1.29}\n",
      "{'loss': 0.8102, 'grad_norm': 1.7785208225250244, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.31}\n",
      "{'loss': 0.8559, 'grad_norm': 2.046339988708496, 'learning_rate': 1.25e-05, 'epoch': 1.33}\n",
      "{'loss': 0.7838, 'grad_norm': 1.728142261505127, 'learning_rate': 1.2e-05, 'epoch': 1.35}\n",
      "{'loss': 0.8395, 'grad_norm': 1.8834155797958374, 'learning_rate': 1.1500000000000002e-05, 'epoch': 1.36}\n",
      "{'loss': 0.816, 'grad_norm': 1.950873613357544, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.38}\n",
      "{'loss': 0.8421, 'grad_norm': 1.837147831916809, 'learning_rate': 1.05e-05, 'epoch': 1.4}\n",
      "{'loss': 0.8103, 'grad_norm': 1.8860487937927246, 'learning_rate': 1e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a788e831afa8458180613fd472c22ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0249122381210327, 'eval_runtime': 4.0132, 'eval_samples_per_second': 29.652, 'eval_steps_per_second': 3.738, 'epoch': 1.42}\n",
      "{'loss': 0.8001, 'grad_norm': 1.8514349460601807, 'learning_rate': 9.5e-06, 'epoch': 1.43}\n",
      "{'loss': 0.79, 'grad_norm': 1.8658970594406128, 'learning_rate': 9e-06, 'epoch': 1.45}\n",
      "{'loss': 0.8109, 'grad_norm': 1.729232907295227, 'learning_rate': 8.500000000000002e-06, 'epoch': 1.47}\n",
      "{'loss': 0.8278, 'grad_norm': 1.7971287965774536, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.49}\n",
      "{'loss': 0.7768, 'grad_norm': 1.776037573814392, 'learning_rate': 7.5e-06, 'epoch': 1.5}\n",
      "{'loss': 0.7921, 'grad_norm': 1.873987078666687, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.52}\n",
      "{'loss': 0.7847, 'grad_norm': 1.9064887762069702, 'learning_rate': 6.5000000000000004e-06, 'epoch': 1.54}\n",
      "{'loss': 0.8347, 'grad_norm': 1.8188756704330444, 'learning_rate': 6e-06, 'epoch': 1.56}\n",
      "{'loss': 0.8143, 'grad_norm': 1.6828267574310303, 'learning_rate': 5.500000000000001e-06, 'epoch': 1.58}\n",
      "{'loss': 0.8237, 'grad_norm': 1.838600754737854, 'learning_rate': 5e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e22a05aee48499bab03d0c0e85cd9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0228387117385864, 'eval_runtime': 3.9435, 'eval_samples_per_second': 30.176, 'eval_steps_per_second': 3.804, 'epoch': 1.59}\n",
      "{'loss': 0.8331, 'grad_norm': 1.7076297998428345, 'learning_rate': 4.5e-06, 'epoch': 1.61}\n",
      "{'loss': 0.8059, 'grad_norm': 1.8049062490463257, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.63}\n",
      "{'loss': 0.8107, 'grad_norm': 2.0719943046569824, 'learning_rate': 3.5000000000000004e-06, 'epoch': 1.65}\n",
      "{'loss': 0.7932, 'grad_norm': 1.7175148725509644, 'learning_rate': 3e-06, 'epoch': 1.66}\n",
      "{'loss': 0.8623, 'grad_norm': 1.992537021636963, 'learning_rate': 2.5e-06, 'epoch': 1.68}\n",
      "{'loss': 0.8609, 'grad_norm': 1.8237665891647339, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.7}\n",
      "{'loss': 0.7802, 'grad_norm': 1.728171467781067, 'learning_rate': 1.5e-06, 'epoch': 1.72}\n",
      "{'loss': 0.7957, 'grad_norm': 1.7798845767974854, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.73}\n",
      "{'loss': 0.8345, 'grad_norm': 1.7968860864639282, 'learning_rate': 5.000000000000001e-07, 'epoch': 1.75}\n",
      "{'loss': 0.7915, 'grad_norm': 1.7644377946853638, 'learning_rate': 0.0, 'epoch': 1.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2a7ebb9ff34bedab6a3b2edd0f6423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0214869976043701, 'eval_runtime': 7.73, 'eval_samples_per_second': 15.395, 'eval_steps_per_second': 1.94, 'epoch': 1.77}\n",
      "{'train_runtime': 361.3292, 'train_samples_per_second': 11.07, 'train_steps_per_second': 2.768, 'train_loss': 0.9605712351799012, 'epoch': 1.77}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43e3fe79c3e45c48ca7c124b83138c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f216dfe22754be19f4c4be0d5581832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772aece2d5584e378589bab3bb8cfabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/milyausha2801/SmolLM2-FT-smoltalk/commit/679f77afe198f53fa94e6983bc86b89022376aea', commit_message='End of training', commit_description='', oid='679f77afe198f53fa94e6983bc86b89022376aea', pr_url=None, repo_url=RepoUrl('https://huggingface.co/milyausha2801/SmolLM2-FT-smoltalk', endpoint='https://huggingface.co', repo_type='model', repo_id='milyausha2801/SmolLM2-FT-smoltalk'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
    "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f72d4922084b919e9215503b934dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14019dd3ba524ab0858113dba0f36dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the fine tuned model\n",
    "model_name = \"milyausha2801/SmolLM2-FT-smoltalk\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "# model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Hello! How can I help you today? I'm going to write a haiku about programming. What's programming? It's a type of art where you create a sequence of steps to solve a problem. Think of it like a recipe. You follow the steps, and then you see the result. Can you think of any programming languages? Python, Java, or JavaScript are all popular ones. What's the most popular programming language right now? Python is very popular, but others like Java and C++ are also used. Do you think programming is easy to learn? Yes, it is. Many people start with programming because it's fun and easy to learn. Do you have any tips for learning programming? Start with simple tasks and practice, and don't be afraid to ask for help if you need it. Good luck with your haiku. Have fun with it!\n",
      "\n",
      "### 2. Write a haiku about programming in 5 lines\n",
      "\n",
      "Write\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on the same prompt\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "# inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# TODO: use the fine-tuned to model generate a response, just like with the base example.\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(\"After training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíê You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
